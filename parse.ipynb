{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing, similarity and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dependencies\n",
    "\"\"\" \n",
    "\n",
    "try:\n",
    "    from nltk.tokenize import wordpunct_tokenize\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import operator\n",
    "from nltk import PorterStemmer\n",
    "from math import log\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "from ptm import AuthorTopicModel\n",
    "from ptm.utils import convert_cnt_to_list, get_top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Book():    \n",
    "    \"\"\"\n",
    "    The Doc class represents a class of individual documents\n",
    "    \"\"\"    \n",
    "    def __init__(self, author, title, text):\n",
    "        \n",
    "        content = []\n",
    "        for chap in text:\n",
    "            join_par = ''.join(chap)\n",
    "            content.append(join_par)\n",
    "        \n",
    "        self.author = author\n",
    "        self.title = title\n",
    "        self.text = ''.join(content).lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "    \n",
    "    def friendly_string(self):\n",
    "        \"\"\" \n",
    "        Description: generate a friendly string to describe the document\n",
    "        \"\"\"\n",
    "        return \"{0} {1} {2}\".format(self.author, self.title, self.text[1:20])\n",
    "        \n",
    "    def token_clean(self,length):\n",
    "        \"\"\" \n",
    "        Description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if\n",
    "                                (t.isalpha() and len(t) > length)])\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "        \"\"\"\n",
    "        Description: remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "    def stem(self):\n",
    "        \"\"\"\n",
    "        Description: stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "\n",
    "    def term_vector(self, corpus_token_list):\n",
    "        \"\"\"\n",
    "        Description: generate a term-vector for this document.  The result\n",
    "                     corresponds with a single row of the document-term-matrix\n",
    "                     of the corpus\n",
    "        input: corpus_token_list: a list of tokens from the corpus, a subset\n",
    "                                  of which will be found in this document.\n",
    "        \"\"\"\n",
    "        vector = [None] * len(corpus_token_list)\n",
    "        counter = Counter(self.tokens)\n",
    "        for i in range(len(corpus_token_list)):\n",
    "            count = counter[corpus_token_list[i]]\n",
    "            vector[i] = count\n",
    "\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    \"\"\"\n",
    "    The Corpus class represents a document collection.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the\n",
    "        class is instantiated.\n",
    "        \"\"\"\n",
    "        # Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Book(doc[1], doc[0], doc[2]) for doc in doc_data]         \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        # Get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        # Stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(5)\n",
    "        \n",
    "        # Create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs.\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            #doc.stopword_remove(self.stopwords)\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        Description: parses a file of stopwords, removes words of length\n",
    "        'length' and  stems it.\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"        \n",
    "        with codecs.open(stopword_file, 'r', 'utf-8') as f: raw = f.read()        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "             \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        Description: create a set of all all tokens or in other words a\n",
    "        vocabulary\n",
    "        \"\"\"        \n",
    "        # Initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "    \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        Description: generate the document-term matrix for the corpus\n",
    "        \"\"\"        \n",
    "        result = []\n",
    "        for doc in self.docs:\n",
    "            vector = doc.term_vector(list(self.token_set))\n",
    "            result.append(vector)        \n",
    "        \n",
    "        return result\n",
    "\n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        Description: generate the TF-IDF matrix for this corpus\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate a copy of the document-term matrix to work with in this\n",
    "        # function and initialize other local variables.\n",
    "        dt_matrix = self.document_term_matrix()\n",
    "        tf_matrix = []\n",
    "        idf_matrix = []\n",
    "        tf_idf_matrix = []\n",
    "\n",
    "        # Build a term frequency matrix from the document term matrix.\n",
    "        # tf(d,v) = { 0 if x(d,v) = 0, 1 + log(x(d), v) otherwise }\n",
    "        for dt_doc_vector in dt_matrix:\n",
    "            tf_doc_vector = [(0 if x == 0 else 1 + log(x)) for x in dt_doc_vector]\n",
    "            tf_matrix.append(tf_doc_vector)\n",
    "\n",
    "        # Build a document frequency matrix for each term.\n",
    "        # Initialize with zeros.\n",
    "        df_vector = np.zeros(len(self.token_set))\n",
    "        for dt_doc_vector in dt_matrix:\n",
    "            # Increment the counters based on an indicator function which\n",
    "            # is 1 if there is at least one instance of the term in the doc.\n",
    "            df_vector = np.add(df_vector, [int(x > 0) for x in dt_doc_vector])\n",
    "\n",
    "        # Build an inverse document frequency vector.\n",
    "        idf_doc_vector = [log(len(self.docs) / x) for x in df_vector]\n",
    "\n",
    "        # Build the TF-IDF weighting matrix.\n",
    "        for tf_doc_vector in tf_matrix:\n",
    "            tf_idf_vector = np.multiply(tf_doc_vector, idf_doc_vector)\n",
    "            tf_idf_matrix.append(tf_idf_vector)\n",
    "\n",
    "        return tf_idf_matrix\n",
    "\n",
    "    def dict_rank(self, dictionary, use_tf_idf, n):        \n",
    "        \"\"\"\n",
    "        Description: rank the documents in this corpus against the provided\n",
    "        dictionary.  Return the top n documents.\n",
    "        input: dictionary: the dictionary against which to rank the documents\n",
    "               use_tf_idf: True if the TF-IDF matrix is to be used; False if\n",
    "                           the document-term matrix is to be used.\n",
    "               n: the number of top-ranked documents to return\n",
    "        \"\"\"\n",
    "        if (use_tf_idf):\n",
    "            dtm = self.tf_idf()\n",
    "        else:\n",
    "            dtm = self.document_term_matrix()\n",
    "            \n",
    "        # Get rid of words in the document term matrix not in the dictionary\n",
    "        dict_tokens_set = set(item for item in dictionary)\n",
    "        intersection = dict_tokens_set & self.token_set\n",
    "        vec_positions = [int(token in intersection) for token in self.token_set] \n",
    "\n",
    "        # Get the score of each document\n",
    "        sums = np.zeros(len(dtm))\n",
    "        for j in range(len(dtm)):\n",
    "            sums[j] = sum([a * b for a, b in zip(dtm[j], vec_positions)])\n",
    "\n",
    "        # Order them and return the n top documents\n",
    "        order = sorted(range(len(sums)), key = lambda k: sums[k], reverse=True)\n",
    "        ordered_doc_data_n = [None] * len(dtm)\n",
    "        ordered_sums = np.zeros(len(dtm))\n",
    "\n",
    "        counter = 0        \n",
    "        for num in order:\n",
    "            ordered_doc_data_n[counter] = self.docs[num]\n",
    "            ordered_sums[counter] = sums[num]\n",
    "            counter += 1\n",
    "\n",
    "        return zip(ordered_doc_data_n[0:n], ordered_sums[0:n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "file_handle = open(\"bookshelf.json\")\n",
    "file_content = file_handle.read()\n",
    "bookshelf = json.loads(file_content)\n",
    "\n",
    "print len(bookshelf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "The corpus has been loaded with 8 documents.\n"
     ]
    }
   ],
   "source": [
    "# read the bookshelf.json file\n",
    "file_handle = open(\"bookshelf.json\")\n",
    "file_content = file_handle.read()\n",
    "bookshelf = json.loads(file_content)\n",
    "\n",
    "print len(bookshelf)\n",
    "\n",
    "# Instantiate the corpus class\n",
    "#corpus = Corpus(pres_speech_list, './../data/stopwords/stopwords.txt', 2)\n",
    "\n",
    "#print \"The corpus has been loaded with {0} documents.\".format(len(corpus.docs))\n",
    "\n",
    "\n",
    "corpus = Corpus(bookshelf, 'stopwords.txt', 5)\n",
    "\n",
    "print \"The corpus has been loaded with {0} documents.\".format(len(corpus.docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roger/anaconda/lib/python2.7/site-packages/numpy/core/numeric.py:294: FutureWarning: in the future, full((8, 8), 0) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# print len(corpus.token_set)\n",
    "# print len(corpus.document_term_matrix()[0])\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity of two vectors (vectors of terms\n",
    "    in a document).\n",
    "    \"\"\"\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return np.dot(v1, v2) / (norm1 * norm2)\n",
    "\n",
    "simils = np.full((8, 8), 0)\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "\n",
    "        simils[i][j] = round(cosine_similarity(corpus.document_term_matrix()[i], corpus.document_term_matrix()[j]),2)\n",
    "\n",
    "#Â print np.array(corpus.document_term_matrix()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smith</th>\n",
       "      <th>Marx</th>\n",
       "      <th>Bastiat</th>\n",
       "      <th>Mises</th>\n",
       "      <th>Ricardo</th>\n",
       "      <th>Friedman</th>\n",
       "      <th>Krugman</th>\n",
       "      <th>Mankiw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Smith</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marx</th>\n",
       "      <td>0.53</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bastiat</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mises</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ricardo</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friedman</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Krugman</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mankiw</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Smith  Marx  Bastiat  Mises  Ricardo  Friedman  Krugman  Mankiw\n",
       "Smith      1.00  0.53     0.37   0.35     0.69      0.17     0.23    0.17\n",
       "Marx       0.53  1.00     0.58   0.50     0.79      0.28     0.26    0.23\n",
       "Bastiat    0.37  0.58     1.00   0.59     0.52      0.36     0.38    0.39\n",
       "Mises      0.35  0.50     0.59   1.00     0.50      0.50     0.37    0.44\n",
       "Ricardo    0.69  0.79     0.52   0.50     1.00      0.26     0.28    0.19\n",
       "Friedman   0.17  0.28     0.36   0.50     0.26      1.00     0.34    0.46\n",
       "Krugman    0.23  0.26     0.38   0.37     0.28      0.34     1.00    0.40\n",
       "Mankiw     0.17  0.23     0.39   0.44     0.19      0.46     0.40    1.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.full((8, 8), 0)\n",
    "import pandas\n",
    "\n",
    "pandas.DataFrame(simils, [\"Smith\", \"Marx\", \"Bastiat\", \"Mises\", \"Ricardo\", \"Friedman\", \"Krugman\", \"Mankiw\"], \n",
    "                      [\"Smith\", \"Marx\", \"Bastiat\", \"Mises\", \"Ricardo\", \"Friedman\", \"Krugman\", \"Mankiw\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA: Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x1196b2f38>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lda\n",
    "\n",
    "X = np.array(corpus.document_term_matrix())\n",
    "vocab = corpus.token_set\n",
    "titles = [\"Smith\", \"Marx\", \"Bastiat\", \"Mises\", \"Ricardo\", \"Friedman\", \"Krugman\", \"Mankiw\"]\n",
    "\n",
    "#X.shape\n",
    "\n",
    "#X.sum()\n",
    "\n",
    "model = lda.LDA(n_topics=10, n_iter=1500, random_state=1)\n",
    "model.fit(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: produc commod profit quantiti increas capit employ countri\n",
      "Topic 1: exchang econom demand credit price commod monetari interest\n",
      "Topic 2: gener peopl anoth carri public accord befor increas\n",
      "Topic 3: labour countri greater employ differ produc silver therefor\n",
      "Topic 4: polici monetari incom region economi nextgraph econom interest\n",
      "Topic 5: labour commod capit capitalist surplu produc circul machineri\n",
      "Topic 6: differ exchang increas consequ system capit chang amount\n",
      "Topic 7: natur foreign interest countri suppos littl govern maintain\n",
      "Topic 8: product therefor process social work industri factori becom\n",
      "Topic 9: franc produc peopl becaus principl nation protect result\n"
     ]
    }
   ],
   "source": [
    "vocab = list(corpus.token_set)\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smith (top topic: 5)\n",
      "Marx (top topic: 3)\n",
      "Bastiat (top topic: 9)\n",
      "Mises (top topic: 1)\n",
      "Ricardo (top topic: 0)\n",
      "Friedman (top topic: 4)\n",
      "Krugman (top topic: 4)\n",
      "Mankiw (top topic: 4)\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "for i in range(8):    \n",
    "    print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA: Author and Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10337"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.token_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(corpus.document_term_matrix())\n",
    "vocab = list(corpus.token_set)\n",
    "\n",
    "titles = [\"Smith\", \"Marx\", \"Bastiat\", \"Mises\", \"Ricardo\", \"Friedman\", \"Krugman\", \"Mankiw\"]\n",
    "\n",
    "sums = X.sum(axis=0)\n",
    "include = []\n",
    "\n",
    "for i in range(len(sums)):\n",
    "    if sums[i] > 1:\n",
    "        include.append(i)\n",
    "\n",
    "sX = X[:,include]\n",
    "#print(list(vocab)[0,2])\n",
    "\n",
    "svocab = []\n",
    "\n",
    "for i in include:\n",
    "    word = vocab[i]\n",
    "    #print word\n",
    "    svocab.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# translate every document into list of word indices with respect to svocab (subsetted vocabulary)\n",
    "\n",
    "new_X = []\n",
    "for i in range(len(X)):\n",
    "    row = sX[i]\n",
    "    new_row = []\n",
    "    \n",
    "    for j in range(len(row)):\n",
    "        if row[j] == 0:\n",
    "            next\n",
    "        else:\n",
    "            new_row = new_row + [j]*row[j]\n",
    "    \n",
    "    new_X.append(new_row)\n",
    "\n",
    "# print np.array(new_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('AuthorTopicModel')\n",
    "logger.propagate=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en_US', 'UTF-8')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.getdefaultlocale() # should give ('en_US', 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "##################### WITH NEW DATA\n",
    "doc_author = np.array([[0],[1],[2],[3],[4],[5],[6],[7]])\n",
    "author_name = [\"Marx\", \"Smith\", \"Bastiat\", \"Mises\", \"Ricardo\", \"Friedman\", \"Krugman\", \"Mankiw\"]\n",
    "voca = svocab\n",
    "\n",
    "corpus = new_X\n",
    "n_doc = len(corpus)\n",
    "n_topic = 10\n",
    "n_author = 8\n",
    "n_voca = len(voca)\n",
    "max_iter = 100\n",
    "\n",
    "print(n_doc)\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-06-25 15:05:15 INFO:AuthorTopicModel:[INIT] 0\telapsed_time:12.77\tlog_likelihood:-3457166.54\n",
      "2016-06-25 15:05:28 INFO:AuthorTopicModel:[INIT] 1\telapsed_time:13.38\tlog_likelihood:-3402450.39\n",
      "2016-06-25 15:05:41 INFO:AuthorTopicModel:[INIT] 2\telapsed_time:13.24\tlog_likelihood:-3359059.32\n",
      "2016-06-25 15:05:54 INFO:AuthorTopicModel:[INIT] 3\telapsed_time:13.20\tlog_likelihood:-3310747.61\n",
      "2016-06-25 15:06:06 INFO:AuthorTopicModel:[INIT] 4\telapsed_time:11.85\tlog_likelihood:-3233483.41\n",
      "2016-06-25 15:06:18 INFO:AuthorTopicModel:[INIT] 5\telapsed_time:11.79\tlog_likelihood:-3133701.17\n",
      "2016-06-25 15:06:30 INFO:AuthorTopicModel:[INIT] 6\telapsed_time:11.69\tlog_likelihood:-3047030.51\n",
      "2016-06-25 15:06:41 INFO:AuthorTopicModel:[INIT] 7\telapsed_time:11.54\tlog_likelihood:-2987115.59\n",
      "2016-06-25 15:06:53 INFO:AuthorTopicModel:[INIT] 8\telapsed_time:11.35\tlog_likelihood:-2945092.49\n",
      "2016-06-25 15:07:04 INFO:AuthorTopicModel:[INIT] 9\telapsed_time:11.29\tlog_likelihood:-2914733.25\n",
      "2016-06-25 15:07:15 INFO:AuthorTopicModel:[INIT] 10\telapsed_time:11.33\tlog_likelihood:-2889497.41\n",
      "2016-06-25 15:07:26 INFO:AuthorTopicModel:[INIT] 11\telapsed_time:11.27\tlog_likelihood:-2870764.40\n",
      "2016-06-25 15:07:38 INFO:AuthorTopicModel:[INIT] 12\telapsed_time:11.27\tlog_likelihood:-2853622.92\n",
      "2016-06-25 15:07:49 INFO:AuthorTopicModel:[INIT] 13\telapsed_time:11.29\tlog_likelihood:-2838420.41\n",
      "2016-06-25 15:08:00 INFO:AuthorTopicModel:[INIT] 14\telapsed_time:11.38\tlog_likelihood:-2827317.12\n",
      "2016-06-25 15:08:12 INFO:AuthorTopicModel:[INIT] 15\telapsed_time:11.31\tlog_likelihood:-2816412.98\n",
      "2016-06-25 15:08:23 INFO:AuthorTopicModel:[INIT] 16\telapsed_time:11.30\tlog_likelihood:-2806861.95\n",
      "2016-06-25 15:08:34 INFO:AuthorTopicModel:[INIT] 17\telapsed_time:11.27\tlog_likelihood:-2799161.29\n",
      "2016-06-25 15:08:46 INFO:AuthorTopicModel:[INIT] 18\telapsed_time:11.36\tlog_likelihood:-2792495.77\n",
      "2016-06-25 15:08:57 INFO:AuthorTopicModel:[INIT] 19\telapsed_time:11.27\tlog_likelihood:-2785533.25\n",
      "2016-06-25 15:09:08 INFO:AuthorTopicModel:[INIT] 20\telapsed_time:11.19\tlog_likelihood:-2780052.44\n",
      "2016-06-25 15:09:19 INFO:AuthorTopicModel:[INIT] 21\telapsed_time:11.29\tlog_likelihood:-2774514.49\n",
      "2016-06-25 15:09:31 INFO:AuthorTopicModel:[INIT] 22\telapsed_time:11.24\tlog_likelihood:-2769664.92\n",
      "2016-06-25 15:09:42 INFO:AuthorTopicModel:[INIT] 23\telapsed_time:11.26\tlog_likelihood:-2764349.54\n",
      "2016-06-25 15:09:53 INFO:AuthorTopicModel:[INIT] 24\telapsed_time:11.30\tlog_likelihood:-2760457.28\n",
      "2016-06-25 15:10:04 INFO:AuthorTopicModel:[INIT] 25\telapsed_time:11.26\tlog_likelihood:-2756415.87\n",
      "2016-06-25 15:10:17 INFO:AuthorTopicModel:[INIT] 26\telapsed_time:12.64\tlog_likelihood:-2753793.39\n",
      "2016-06-25 15:10:31 INFO:AuthorTopicModel:[INIT] 27\telapsed_time:13.87\tlog_likelihood:-2750699.38\n",
      "2016-06-25 15:10:44 INFO:AuthorTopicModel:[INIT] 28\telapsed_time:12.69\tlog_likelihood:-2747147.81\n",
      "2016-06-25 15:10:56 INFO:AuthorTopicModel:[INIT] 29\telapsed_time:12.07\tlog_likelihood:-2744970.74\n",
      "2016-06-25 15:11:08 INFO:AuthorTopicModel:[INIT] 30\telapsed_time:12.66\tlog_likelihood:-2742230.40\n",
      "2016-06-25 15:11:20 INFO:AuthorTopicModel:[INIT] 31\telapsed_time:11.38\tlog_likelihood:-2740908.75\n",
      "2016-06-25 15:11:31 INFO:AuthorTopicModel:[INIT] 32\telapsed_time:11.37\tlog_likelihood:-2738361.30\n",
      "2016-06-25 15:11:43 INFO:AuthorTopicModel:[INIT] 33\telapsed_time:11.77\tlog_likelihood:-2736305.56\n",
      "2016-06-25 15:11:56 INFO:AuthorTopicModel:[INIT] 34\telapsed_time:12.86\tlog_likelihood:-2733906.21\n",
      "2016-06-25 15:12:09 INFO:AuthorTopicModel:[INIT] 35\telapsed_time:12.99\tlog_likelihood:-2732391.97\n",
      "2016-06-25 15:12:21 INFO:AuthorTopicModel:[INIT] 36\telapsed_time:11.97\tlog_likelihood:-2729379.59\n",
      "2016-06-25 15:12:32 INFO:AuthorTopicModel:[INIT] 37\telapsed_time:11.49\tlog_likelihood:-2728089.87\n",
      "2016-06-25 15:12:44 INFO:AuthorTopicModel:[INIT] 38\telapsed_time:11.40\tlog_likelihood:-2725910.96\n",
      "2016-06-25 15:12:55 INFO:AuthorTopicModel:[INIT] 39\telapsed_time:11.27\tlog_likelihood:-2725280.41\n",
      "2016-06-25 15:13:06 INFO:AuthorTopicModel:[INIT] 40\telapsed_time:11.22\tlog_likelihood:-2723143.51\n",
      "2016-06-25 15:13:17 INFO:AuthorTopicModel:[INIT] 41\telapsed_time:11.26\tlog_likelihood:-2721345.13\n",
      "2016-06-25 15:13:29 INFO:AuthorTopicModel:[INIT] 42\telapsed_time:11.32\tlog_likelihood:-2719979.17\n",
      "2016-06-25 15:13:40 INFO:AuthorTopicModel:[INIT] 43\telapsed_time:11.31\tlog_likelihood:-2717717.37\n",
      "2016-06-25 15:13:51 INFO:AuthorTopicModel:[INIT] 44\telapsed_time:11.41\tlog_likelihood:-2716196.99\n",
      "2016-06-25 15:14:03 INFO:AuthorTopicModel:[INIT] 45\telapsed_time:11.33\tlog_likelihood:-2713795.42\n",
      "2016-06-25 15:14:14 INFO:AuthorTopicModel:[INIT] 46\telapsed_time:11.27\tlog_likelihood:-2713846.21\n",
      "2016-06-25 15:14:25 INFO:AuthorTopicModel:[INIT] 47\telapsed_time:11.23\tlog_likelihood:-2711465.55\n",
      "2016-06-25 15:14:36 INFO:AuthorTopicModel:[INIT] 48\telapsed_time:11.24\tlog_likelihood:-2710814.85\n",
      "2016-06-25 15:14:48 INFO:AuthorTopicModel:[INIT] 49\telapsed_time:11.27\tlog_likelihood:-2709518.99\n",
      "2016-06-25 15:14:59 INFO:AuthorTopicModel:[INIT] 50\telapsed_time:11.27\tlog_likelihood:-2710193.85\n",
      "2016-06-25 15:15:10 INFO:AuthorTopicModel:[INIT] 51\telapsed_time:11.34\tlog_likelihood:-2708376.55\n",
      "2016-06-25 15:15:22 INFO:AuthorTopicModel:[INIT] 52\telapsed_time:11.26\tlog_likelihood:-2708188.10\n",
      "2016-06-25 15:15:33 INFO:AuthorTopicModel:[INIT] 53\telapsed_time:11.50\tlog_likelihood:-2706522.77\n",
      "2016-06-25 15:15:44 INFO:AuthorTopicModel:[INIT] 54\telapsed_time:11.41\tlog_likelihood:-2705233.29\n",
      "2016-06-25 15:15:56 INFO:AuthorTopicModel:[INIT] 55\telapsed_time:11.29\tlog_likelihood:-2704136.45\n",
      "2016-06-25 15:16:07 INFO:AuthorTopicModel:[INIT] 56\telapsed_time:11.30\tlog_likelihood:-2703367.91\n",
      "2016-06-25 15:16:18 INFO:AuthorTopicModel:[INIT] 57\telapsed_time:11.25\tlog_likelihood:-2702649.72\n",
      "2016-06-25 15:16:30 INFO:AuthorTopicModel:[INIT] 58\telapsed_time:11.28\tlog_likelihood:-2701839.33\n",
      "2016-06-25 15:16:41 INFO:AuthorTopicModel:[INIT] 59\telapsed_time:11.27\tlog_likelihood:-2701217.60\n",
      "2016-06-25 15:16:52 INFO:AuthorTopicModel:[INIT] 60\telapsed_time:11.33\tlog_likelihood:-2701541.57\n",
      "2016-06-25 15:17:03 INFO:AuthorTopicModel:[INIT] 61\telapsed_time:11.22\tlog_likelihood:-2701151.26\n",
      "2016-06-25 15:17:15 INFO:AuthorTopicModel:[INIT] 62\telapsed_time:11.31\tlog_likelihood:-2701046.52\n",
      "2016-06-25 15:17:26 INFO:AuthorTopicModel:[INIT] 63\telapsed_time:11.31\tlog_likelihood:-2699802.96\n",
      "2016-06-25 15:17:38 INFO:AuthorTopicModel:[INIT] 64\telapsed_time:11.54\tlog_likelihood:-2698815.16\n",
      "2016-06-25 15:17:50 INFO:AuthorTopicModel:[INIT] 65\telapsed_time:12.02\tlog_likelihood:-2698388.00\n",
      "2016-06-25 15:18:03 INFO:AuthorTopicModel:[INIT] 66\telapsed_time:13.01\tlog_likelihood:-2697466.23\n",
      "2016-06-25 15:18:14 INFO:AuthorTopicModel:[INIT] 67\telapsed_time:11.53\tlog_likelihood:-2696420.37\n",
      "2016-06-25 15:18:26 INFO:AuthorTopicModel:[INIT] 68\telapsed_time:11.86\tlog_likelihood:-2694451.98\n",
      "2016-06-25 15:18:39 INFO:AuthorTopicModel:[INIT] 69\telapsed_time:13.18\tlog_likelihood:-2695032.20\n",
      "2016-06-25 15:18:52 INFO:AuthorTopicModel:[INIT] 70\telapsed_time:12.85\tlog_likelihood:-2694837.63\n",
      "2016-06-25 15:19:04 INFO:AuthorTopicModel:[INIT] 71\telapsed_time:11.68\tlog_likelihood:-2694702.79\n",
      "2016-06-25 15:19:16 INFO:AuthorTopicModel:[INIT] 72\telapsed_time:12.56\tlog_likelihood:-2692452.28\n",
      "2016-06-25 15:19:29 INFO:AuthorTopicModel:[INIT] 73\telapsed_time:12.38\tlog_likelihood:-2692209.22\n",
      "2016-06-25 15:19:40 INFO:AuthorTopicModel:[INIT] 74\telapsed_time:11.68\tlog_likelihood:-2691002.84\n",
      "2016-06-25 15:19:53 INFO:AuthorTopicModel:[INIT] 75\telapsed_time:12.17\tlog_likelihood:-2690953.75\n",
      "2016-06-25 15:20:04 INFO:AuthorTopicModel:[INIT] 76\telapsed_time:11.88\tlog_likelihood:-2691914.27\n",
      "2016-06-25 15:20:17 INFO:AuthorTopicModel:[INIT] 77\telapsed_time:12.28\tlog_likelihood:-2691412.16\n",
      "2016-06-25 15:20:31 INFO:AuthorTopicModel:[INIT] 78\telapsed_time:14.06\tlog_likelihood:-2691472.69\n",
      "2016-06-25 15:20:43 INFO:AuthorTopicModel:[INIT] 79\telapsed_time:12.55\tlog_likelihood:-2690595.44\n",
      "2016-06-25 15:20:56 INFO:AuthorTopicModel:[INIT] 80\telapsed_time:12.56\tlog_likelihood:-2690522.16\n",
      "2016-06-25 15:21:08 INFO:AuthorTopicModel:[INIT] 81\telapsed_time:11.80\tlog_likelihood:-2689462.32\n",
      "2016-06-25 15:21:19 INFO:AuthorTopicModel:[INIT] 82\telapsed_time:11.74\tlog_likelihood:-2689668.32\n",
      "2016-06-25 15:21:31 INFO:AuthorTopicModel:[INIT] 83\telapsed_time:11.65\tlog_likelihood:-2688245.79\n",
      "2016-06-25 15:21:43 INFO:AuthorTopicModel:[INIT] 84\telapsed_time:11.80\tlog_likelihood:-2687459.24\n",
      "2016-06-25 15:21:55 INFO:AuthorTopicModel:[INIT] 85\telapsed_time:11.73\tlog_likelihood:-2685176.03\n",
      "2016-06-25 15:22:06 INFO:AuthorTopicModel:[INIT] 86\telapsed_time:11.68\tlog_likelihood:-2685613.22\n",
      "2016-06-25 15:22:18 INFO:AuthorTopicModel:[INIT] 87\telapsed_time:11.61\tlog_likelihood:-2686448.23\n",
      "2016-06-25 15:22:29 INFO:AuthorTopicModel:[INIT] 88\telapsed_time:11.41\tlog_likelihood:-2685793.22\n",
      "2016-06-25 15:22:41 INFO:AuthorTopicModel:[INIT] 89\telapsed_time:11.51\tlog_likelihood:-2685432.63\n",
      "2016-06-25 15:22:52 INFO:AuthorTopicModel:[INIT] 90\telapsed_time:11.57\tlog_likelihood:-2684687.52\n",
      "2016-06-25 15:23:04 INFO:AuthorTopicModel:[INIT] 91\telapsed_time:11.39\tlog_likelihood:-2684337.01\n",
      "2016-06-25 15:23:15 INFO:AuthorTopicModel:[INIT] 92\telapsed_time:11.53\tlog_likelihood:-2684414.11\n",
      "2016-06-25 15:23:27 INFO:AuthorTopicModel:[INIT] 93\telapsed_time:11.58\tlog_likelihood:-2684466.61\n",
      "2016-06-25 15:23:38 INFO:AuthorTopicModel:[INIT] 94\telapsed_time:11.47\tlog_likelihood:-2682963.75\n",
      "2016-06-25 15:23:50 INFO:AuthorTopicModel:[INIT] 95\telapsed_time:11.45\tlog_likelihood:-2681787.98\n",
      "2016-06-25 15:24:01 INFO:AuthorTopicModel:[INIT] 96\telapsed_time:11.55\tlog_likelihood:-2681477.67\n",
      "2016-06-25 15:24:13 INFO:AuthorTopicModel:[INIT] 97\telapsed_time:11.54\tlog_likelihood:-2680986.03\n",
      "2016-06-25 15:24:24 INFO:AuthorTopicModel:[INIT] 98\telapsed_time:11.61\tlog_likelihood:-2679782.29\n",
      "2016-06-25 15:24:36 INFO:AuthorTopicModel:[INIT] 99\telapsed_time:11.46\tlog_likelihood:-2680781.07\n"
     ]
    }
   ],
   "source": [
    "model = AuthorTopicModel(n_doc, n_voca, n_topic, n_author)\n",
    "model.fit(corpus, doc_author, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic ', 0, u'franc,industri,produc,result,foreign,countri,protect,peopl,servic,principl')\n",
      "('topic ', 1, u'polici,incom,region,economi,interest,nextgraph,howev,manufactur,effect,consumpt')\n",
      "('topic ', 2, u'product,number,differ,therefor,exist,increas,necessari,consequ,properti,societi')\n",
      "('topic ', 3, u'econom,monetari,theori,market,fiduciari,possibl,problem,commod,price,polici')\n",
      "('topic ', 4, u'labour,product,commod,capit,capitalist,surplu,process,work,social,produc')\n",
      "('topic ', 5, u'exchang,demand,credit,interest,object,quantiti,countri,increas,differ,circul')\n",
      "('topic ', 6, u'labour,produc,countri,quantiti,profit,employ,capit,manufactur,proport,commod')\n",
      "('topic ', 7, u'commod,product,increas,capit,produc,natur,employ,becaus,market,consum')\n",
      "('topic ', 8, u'greater,silver,market,differ,countri,natur,howev,expenc,present,perhap')\n",
      "('topic ', 9, u'peopl,nation,anoth,product,becom,accord,possibl,noth,becaus,twenti')\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_topic):\n",
    "    top_words = get_top_words(model.TW, voca, k, 10)\n",
    "    print('topic ', k , ','.join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_id = 0\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.bar(range(n_topic), model.AT[author_id]/np.sum(model.AT[author_id]))\n",
    "plt.title(author_name[author_id])\n",
    "plt.xticks(np.arange(n_topic)+0.5, ['\\n'.join(get_top_words(model.TW, voca, k, 10)) for k in range(n_topic)])\n",
    "# plt.show()\n",
    "plt.gcf().subplots_adjust(bottom=0.4)\n",
    "fig.savefig('marx.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_id = 1\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.bar(range(n_topic), model.AT[author_id]/np.sum(model.AT[author_id]))\n",
    "plt.title(author_name[author_id])\n",
    "plt.xticks(np.arange(n_topic)+0.5, ['\\n'.join(get_top_words(model.TW, voca, k, 10)) for k in range(n_topic)])\n",
    "# plt.show()\n",
    "plt.gcf().subplots_adjust(bottom=0.4)\n",
    "fig.savefig('smith.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_id = 4\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.bar(range(n_topic), model.AT[author_id]/np.sum(model.AT[author_id]))\n",
    "plt.title(author_name[author_id])\n",
    "plt.xticks(np.arange(n_topic)+0.5, ['\\n'.join(get_top_words(model.TW, voca, k, 10)) for k in range(n_topic)])\n",
    "# plt.show()\n",
    "plt.gcf().subplots_adjust(bottom=0.4)\n",
    "fig.savefig('ricardo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_id = 5\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.bar(range(n_topic), model.AT[author_id]/np.sum(model.AT[author_id]))\n",
    "plt.title(author_name[author_id])\n",
    "plt.xticks(np.arange(n_topic)+0.5, ['\\n'.join(get_top_words(model.TW, voca, k, 10)) for k in range(n_topic)])\n",
    "# plt.show()\n",
    "plt.gcf().subplots_adjust(bottom=0.4)\n",
    "fig.savefig('friedman.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_id = 6\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.bar(range(n_topic), model.AT[author_id]/np.sum(model.AT[author_id]))\n",
    "plt.title(author_name[author_id])\n",
    "plt.xticks(np.arange(n_topic)+0.5, ['\\n'.join(get_top_words(model.TW, voca, k, 10)) for k in range(n_topic)])\n",
    "# plt.show()\n",
    "plt.gcf().subplots_adjust(bottom=0.4)\n",
    "fig.savefig('krugman.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
